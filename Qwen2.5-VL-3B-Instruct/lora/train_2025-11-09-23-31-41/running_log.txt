[INFO|2025-11-09 23:33:54] tokenization_utils_base.py:2060 >> loading file vocab.json from cache at /home/zct/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/vocab.json

[INFO|2025-11-09 23:33:54] tokenization_utils_base.py:2060 >> loading file merges.txt from cache at /home/zct/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/merges.txt

[INFO|2025-11-09 23:33:54] tokenization_utils_base.py:2060 >> loading file tokenizer.json from cache at /home/zct/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/tokenizer.json

[INFO|2025-11-09 23:33:54] tokenization_utils_base.py:2060 >> loading file added_tokens.json from cache at None

[INFO|2025-11-09 23:33:54] tokenization_utils_base.py:2060 >> loading file special_tokens_map.json from cache at None

[INFO|2025-11-09 23:33:54] tokenization_utils_base.py:2060 >> loading file tokenizer_config.json from cache at /home/zct/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/tokenizer_config.json

[INFO|2025-11-09 23:33:54] tokenization_utils_base.py:2060 >> loading file chat_template.jinja from cache at None

[INFO|2025-11-09 23:33:54] tokenization_utils_base.py:2323 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

[INFO|2025-11-09 23:33:56] image_processing_base.py:380 >> loading configuration file preprocessor_config.json from cache at /home/zct/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/preprocessor_config.json

[INFO|2025-11-09 23:33:57] image_processing_base.py:380 >> loading configuration file preprocessor_config.json from cache at /home/zct/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/preprocessor_config.json

[INFO|2025-11-09 23:33:57] image_processing_base.py:433 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}


[INFO|2025-11-09 23:33:57] tokenization_utils_base.py:2060 >> loading file vocab.json from cache at /home/zct/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/vocab.json

[INFO|2025-11-09 23:33:57] tokenization_utils_base.py:2060 >> loading file merges.txt from cache at /home/zct/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/merges.txt

[INFO|2025-11-09 23:33:57] tokenization_utils_base.py:2060 >> loading file tokenizer.json from cache at /home/zct/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/tokenizer.json

[INFO|2025-11-09 23:33:57] tokenization_utils_base.py:2060 >> loading file added_tokens.json from cache at None

[INFO|2025-11-09 23:33:57] tokenization_utils_base.py:2060 >> loading file special_tokens_map.json from cache at None

[INFO|2025-11-09 23:33:57] tokenization_utils_base.py:2060 >> loading file tokenizer_config.json from cache at /home/zct/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/tokenizer_config.json

[INFO|2025-11-09 23:33:57] tokenization_utils_base.py:2060 >> loading file chat_template.jinja from cache at None

[INFO|2025-11-09 23:33:57] tokenization_utils_base.py:2323 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

[INFO|2025-11-09 23:33:59] processing_utils.py:884 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-3B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)

{
  "processor_class": "Qwen2_5_VLProcessor"
}


[INFO|2025-11-09 23:33:59] logging.py:143 >> Loading dataset sharegpt.json...

[INFO|2025-11-09 23:34:13] configuration_utils.py:693 >> loading configuration file config.json from cache at /home/zct/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/config.json

[INFO|2025-11-09 23:34:13] configuration_utils.py:765 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 128000,
  "max_window_layers": 70,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 16,
  "num_hidden_layers": 36,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 2048,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}


[INFO|2025-11-09 23:34:13] logging.py:143 >> Quantizing model to 4 bit with bitsandbytes.

[INFO|2025-11-09 23:34:13] logging.py:143 >> KV cache is disabled during training.

[INFO|2025-11-09 23:34:14] modeling_utils.py:1124 >> loading weights file model.safetensors from cache at /home/zct/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/model.safetensors.index.json

[INFO|2025-11-09 23:34:14] modeling_utils.py:2167 >> Instantiating Qwen2_5_VLForConditionalGeneration model under default dtype torch.bfloat16.

[INFO|2025-11-09 23:34:14] configuration_utils.py:1142 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}


[INFO|2025-11-09 23:34:14] modeling_utils.py:2167 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.bfloat16.

[INFO|2025-11-09 23:34:22] modeling_utils.py:4930 >> All model checkpoint weights were used when initializing Qwen2_5_VLForConditionalGeneration.


[INFO|2025-11-09 23:34:22] modeling_utils.py:4938 >> All the weights of Qwen2_5_VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2.5-VL-3B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5_VLForConditionalGeneration for predictions without further training.

[INFO|2025-11-09 23:34:22] configuration_utils.py:1097 >> loading configuration file generation_config.json from cache at /home/zct/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/generation_config.json

[INFO|2025-11-09 23:34:22] configuration_utils.py:1142 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}


[INFO|2025-11-09 23:34:22] logging.py:143 >> Gradient checkpointing enabled.

[INFO|2025-11-09 23:34:22] logging.py:143 >> Casting multimodal projector outputs in torch.bfloat16.

[INFO|2025-11-09 23:34:22] logging.py:143 >> Using torch SDPA for faster training and inference.

[INFO|2025-11-09 23:34:22] logging.py:143 >> Upcasting trainable params to float32.

[INFO|2025-11-09 23:34:22] logging.py:143 >> Fine-tuning method: LoRA

[INFO|2025-11-09 23:34:22] logging.py:143 >> Found linear modules: v_proj,o_proj,k_proj,up_proj,q_proj,gate_proj,down_proj

[INFO|2025-11-09 23:34:22] logging.py:143 >> Set vision model not trainable: ['visual.patch_embed', 'visual.blocks'].

[INFO|2025-11-09 23:34:22] logging.py:143 >> Set multi model projector not trainable: visual.merger.

[INFO|2025-11-09 23:34:23] logging.py:143 >> trainable params: 14,966,784 || all params: 3,769,589,760 || trainable%: 0.3970

[INFO|2025-11-09 23:34:23] trainer.py:748 >> Using auto half precision backend

[INFO|2025-11-09 23:34:28] trainer.py:2414 >> ***** Running training *****

[INFO|2025-11-09 23:34:28] trainer.py:2415 >>   Num examples = 478

[INFO|2025-11-09 23:34:28] trainer.py:2416 >>   Num Epochs = 3

[INFO|2025-11-09 23:34:28] trainer.py:2417 >>   Instantaneous batch size per device = 1

[INFO|2025-11-09 23:34:28] trainer.py:2420 >>   Total train batch size (w. parallel, distributed & accumulation) = 32

[INFO|2025-11-09 23:34:28] trainer.py:2421 >>   Gradient Accumulation steps = 8

[INFO|2025-11-09 23:34:28] trainer.py:2422 >>   Total optimization steps = 45

[INFO|2025-11-09 23:34:28] trainer.py:2423 >>   Number of trainable parameters = 14,966,784

[INFO|2025-11-09 23:36:28] logging.py:143 >> {'loss': 1.2295, 'learning_rate': 4.9032e-05, 'epoch': 0.33, 'throughput': 3812.70}

[INFO|2025-11-09 23:38:26] logging.py:143 >> {'loss': 1.1837, 'learning_rate': 4.5225e-05, 'epoch': 0.67, 'throughput': 3854.09}

[INFO|2025-11-09 23:40:16] logging.py:143 >> {'loss': 1.1898, 'learning_rate': 3.8980e-05, 'epoch': 1.00, 'throughput': 3887.13}

[INFO|2025-11-09 23:42:11] logging.py:143 >> {'loss': 1.1340, 'learning_rate': 3.1048e-05, 'epoch': 1.33, 'throughput': 3871.25}

[INFO|2025-11-09 23:44:14] logging.py:143 >> {'loss': 1.1077, 'learning_rate': 2.2387e-05, 'epoch': 1.67, 'throughput': 3877.62}

[INFO|2025-11-09 23:46:03] logging.py:143 >> {'loss': 1.0922, 'learning_rate': 1.4041e-05, 'epoch': 2.00, 'throughput': 3899.58}

[INFO|2025-11-09 23:47:59] logging.py:143 >> {'loss': 1.0757, 'learning_rate': 7.0165e-06, 'epoch': 2.33, 'throughput': 3894.42}

[INFO|2025-11-09 23:49:55] logging.py:143 >> {'loss': 1.0694, 'learning_rate': 2.1614e-06, 'epoch': 2.67, 'throughput': 3896.63}

[INFO|2025-11-09 23:51:48] logging.py:143 >> {'loss': 1.0934, 'learning_rate': 6.0899e-08, 'epoch': 3.00, 'throughput': 3903.85}

[INFO|2025-11-09 23:51:48] trainer.py:3984 >> Saving model checkpoint to saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-11-09-23-31-41/checkpoint-45

[INFO|2025-11-09 23:51:51] configuration_utils.py:693 >> loading configuration file config.json from cache at /home/zct/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/config.json

[INFO|2025-11-09 23:51:51] configuration_utils.py:765 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 128000,
  "max_window_layers": 70,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 16,
  "num_hidden_layers": 36,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 2048,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}


[INFO|2025-11-09 23:51:51] tokenization_utils_base.py:2510 >> tokenizer config file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-11-09-23-31-41/checkpoint-45/tokenizer_config.json

[INFO|2025-11-09 23:51:51] tokenization_utils_base.py:2519 >> Special tokens file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-11-09-23-31-41/checkpoint-45/special_tokens_map.json

[INFO|2025-11-09 23:51:51] image_processing_base.py:260 >> Image processor saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-11-09-23-31-41/checkpoint-45/preprocessor_config.json

[INFO|2025-11-09 23:51:51] tokenization_utils_base.py:2510 >> tokenizer config file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-11-09-23-31-41/checkpoint-45/tokenizer_config.json

[INFO|2025-11-09 23:51:51] tokenization_utils_base.py:2519 >> Special tokens file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-11-09-23-31-41/checkpoint-45/special_tokens_map.json

[INFO|2025-11-09 23:51:52] processing_utils.py:648 >> chat template saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-11-09-23-31-41/checkpoint-45/chat_template.json

[INFO|2025-11-09 23:51:52] trainer.py:2681 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)



[INFO|2025-11-09 23:51:52] image_processing_base.py:260 >> Image processor saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-11-09-23-31-41/preprocessor_config.json

[INFO|2025-11-09 23:51:52] tokenization_utils_base.py:2510 >> tokenizer config file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-11-09-23-31-41/tokenizer_config.json

[INFO|2025-11-09 23:51:52] tokenization_utils_base.py:2519 >> Special tokens file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-11-09-23-31-41/special_tokens_map.json

[INFO|2025-11-09 23:51:52] processing_utils.py:648 >> chat template saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-11-09-23-31-41/chat_template.json

[INFO|2025-11-09 23:51:52] trainer.py:3984 >> Saving model checkpoint to saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-11-09-23-31-41

[INFO|2025-11-09 23:51:53] configuration_utils.py:693 >> loading configuration file config.json from cache at /home/zct/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/config.json

[INFO|2025-11-09 23:51:53] configuration_utils.py:765 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 128000,
  "max_window_layers": 70,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 16,
  "num_hidden_layers": 36,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 2048,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}


[INFO|2025-11-09 23:51:53] tokenization_utils_base.py:2510 >> tokenizer config file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-11-09-23-31-41/tokenizer_config.json

[INFO|2025-11-09 23:51:53] tokenization_utils_base.py:2519 >> Special tokens file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-11-09-23-31-41/special_tokens_map.json

[WARNING|2025-11-09 23:51:54] logging.py:148 >> No metric eval_loss to plot.

[WARNING|2025-11-09 23:51:54] logging.py:148 >> No metric eval_accuracy to plot.

[INFO|2025-11-09 23:51:54] trainer.py:4307 >> 
***** Running Evaluation *****

[INFO|2025-11-09 23:51:54] trainer.py:4309 >>   Num examples = 85

[INFO|2025-11-09 23:51:54] trainer.py:4312 >>   Batch size = 1

[INFO|2025-11-09 23:52:07] modelcard.py:450 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}

